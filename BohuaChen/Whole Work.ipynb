{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08f516f5",
   "metadata": {},
   "source": [
    "# Counterfactual WorkFlow\n",
    "\n",
    "1. Caculate importance of words\n",
    "2. Load masked Language Model\n",
    "3. Generate CF dataset with masked Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9096b148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all pacakages used in teacher's scripts\n",
    "import dataclasses\n",
    "import shutil\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from enum import Enum\n",
    "import json\n",
    "\n",
    "import wandb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    precision_recall_fscore_support,\n",
    "    multilabel_confusion_matrix,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    balanced_accuracy_score,\n",
    "    roc_auc_score,\n",
    "    average_precision_score, matthews_corrcoef\n",
    ")\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.utils import compute_class_weight\n",
    "\n",
    "import torch\n",
    "from sklearn.utils.extmath import softmax\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "import transformers\n",
    "import transformers.adapters.composition as ac\n",
    "from transformers import (\n",
    "    AdapterConfig,\n",
    "    AdapterTrainer,\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    EvalPrediction,\n",
    "    EarlyStoppingCallback,\n",
    "    HfArgumentParser,\n",
    "    MultiLingAdapterArguments,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    "    set_seed, TrainerCallback, XLMRobertaTokenizer,\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint, is_main_process\n",
    "from transformers.utils import check_min_version\n",
    "\n",
    "from root import DATA_DIR, AUGMENTED_DIR\n",
    "from utils.custom_callbacks import CustomWandbCallback\n",
    "from long import LongBert\n",
    "from arguments.data_arguments import DataArguments, ProblemType, SegmentationType, DataAugmentationType, LegalArea, \\\n",
    "    OriginCanton, SubDataset, OriginRegion, Jurisdiction\n",
    "from hierarchical.hier_bert.configuration_hier_bert import HierBertConfig\n",
    "from hierarchical.hier_bert.modeling_hier_bert import HierBertForSequenceClassification\n",
    "from hierarchical.hier_camembert.configuration_hier_camembert import HierCamembertConfig\n",
    "from hierarchical.hier_camembert.modeling_hier_camembert import HierCamembertForSequenceClassification\n",
    "from hierarchical.hier_roberta.configuration_hier_roberta import HierRobertaConfig\n",
    "from hierarchical.hier_roberta.modeling_hier_roberta import HierRobertaForSequenceClassification\n",
    "from hierarchical.hier_xlm_roberta.configuration_hier_xlm_roberta import HierXLMRobertaConfig\n",
    "from hierarchical.hier_xlm_roberta.modeling_hier_xlm_roberta import HierXLMRobertaForSequenceClassification\n",
    "from arguments.model_arguments import ModelArguments, LabelImbalanceMethod, LongInputBertType, TrainType\n",
    "from utils.sentencizer import get_sentencizer, combine_small_sentences, spacy_sentencize, get_spacy_sents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7c90f6",
   "metadata": {},
   "source": [
    "## Caculate importance of words\n",
    "1. load model\n",
    "2. load & preprocess data\n",
    "3. calculate importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e63ceb8",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2af97113",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline():\n",
    "    with open(DATA_DIR / 'de' / 'labels.json', 'r') as f:\n",
    "        label_dict = json.load(f)\n",
    "        label_dict['id2label'] = {int(k): v for k, v in label_dict['id2label'].items()}\n",
    "        label_dict['label2id'] = {k: int(v) for k, v in label_dict['label2id'].items()}\n",
    "        label_list = list(label_dict[\"label2id\"].keys())\n",
    "    num_labels = len(label_list)\n",
    "\n",
    "    model_class = AutoModelForSequenceClassification\n",
    "\n",
    "    config = AutoConfig.from_pretrained(\n",
    "                    'xlm-roberta-base',\n",
    "                    num_labels=num_labels,\n",
    "                    id2label=label_dict[\"id2label\"],\n",
    "                    label2id=label_dict[\"label2id\"],\n",
    "                    finetuning_task=\"text-classification\",\n",
    "                    problem_type='single_label_classification',\n",
    "                    cache_dir=None,\n",
    "                    revision='main',\n",
    "                    use_auth_token=None,\n",
    "                    max_segments=4,\n",
    "                    max_segment_length=512,\n",
    "                    segment_encoder_type=\"transformer\",\n",
    "                )\n",
    "\n",
    "\n",
    "    config_class = HierXLMRobertaConfig\n",
    "    model_class = HierXLMRobertaForSequenceClassification\n",
    "    config = config_class(**config.to_dict())\n",
    "\n",
    "\n",
    "    model = model_class.from_pretrained(\n",
    "        'xlm-roberta-base',\n",
    "        from_tf=bool(\".ckpt\" in 'xlm-roberta-base'),\n",
    "        config=config,\n",
    "        cache_dir=None,\n",
    "        revision='main',\n",
    "        use_auth_token=None,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def load_model(model, folder):\n",
    "    model_path = Path(f'{folder}/model.bin')\n",
    "    if model_path.exists():\n",
    "        model.load_state_dict(torch.load(model_path, map_location='cuda:1'))\n",
    "        model.to('cuda:1')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa364a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing HierXLMRobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing HierXLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing HierXLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of HierXLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['segment_encoder.norm.bias', 'seg_pos_embeddings.weight', 'segment_encoder.layers.0.linear1.weight', 'segment_encoder.layers.1.self_attn.out_proj.weight', 'segment_encoder.layers.0.self_attn.in_proj_bias', 'segment_encoder.layers.0.self_attn.out_proj.weight', 'classifier.bias', 'segment_encoder.layers.1.linear1.bias', 'segment_encoder.layers.1.linear2.weight', 'segment_encoder.layers.1.norm1.bias', 'segment_encoder.layers.0.norm1.bias', 'segment_encoder.layers.1.self_attn.in_proj_weight', 'segment_encoder.layers.1.self_attn.out_proj.bias', 'segment_encoder.layers.0.linear2.weight', 'segment_encoder.norm.weight', 'segment_encoder.layers.0.linear2.bias', 'segment_encoder.layers.1.linear1.weight', 'segment_encoder.layers.0.norm1.weight', 'segment_encoder.layers.1.self_attn.in_proj_bias', 'classifier.weight', 'segment_encoder.layers.0.self_attn.out_proj.bias', 'segment_encoder.layers.0.norm2.bias', 'segment_encoder.layers.1.norm2.bias', 'segment_encoder.layers.1.norm1.weight', 'segment_encoder.layers.0.linear1.bias', 'segment_encoder.layers.1.norm2.weight', 'segment_encoder.layers.1.linear2.bias', 'segment_encoder.layers.0.norm2.weight', 'segment_encoder.layers.0.self_attn.in_proj_weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = baseline()\n",
    "load_model(model, 'sjp/teacher_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cbb3ce",
   "metadata": {},
   "source": [
    "### Load And Preprocess Data\n",
    "To simplify my work and reduce running time, I use only France training dataset throughout this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d9ea0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-949afda011810beb\n",
      "Reusing dataset csv (/home/xxkx236/.cache/huggingface/datasets/csv/default-949afda011810beb/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88223889a4ac46458cce71a5daba0b06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_data(lang='fr'):\n",
    "    path = (DATA_DIR / lang / 'train.csv').as_posix()\n",
    "    dataset = load_dataset(\"csv\", data_files={'train': path})['train']\n",
    "    return dataset\n",
    "\n",
    "datasets = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50a4898d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba9d8019243a4889a571291e8e8dd6b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "        'xlm-roberta-base',\n",
    "        do_lower_case=False,\n",
    "        cache_dir=None,\n",
    "        use_fast=True,\n",
    "        revision='main',\n",
    "        use_auth_token=None,\n",
    "    )\n",
    "\n",
    "def append_zero_segments(case_encodings, pad_token_id):\n",
    "    \"\"\"appends a list of zero segments to the encodings to make up for missing segments\"\"\"\n",
    "    return case_encodings + [[pad_token_id] * 512] * (\n",
    "            4 - len(case_encodings))\n",
    "\n",
    "def preprocess_function(batch):\n",
    "    with open(DATA_DIR / 'de' / 'labels.json', 'r') as f:\n",
    "        label_dict = json.load(f)\n",
    "        label_dict['id2label'] = {int(k): v for k, v in label_dict['id2label'].items()}\n",
    "        label_dict['label2id'] = {k: int(v) for k, v in label_dict['label2id'].items()}\n",
    "        label_list = list(label_dict[\"label2id\"].keys())\n",
    "    num_labels = len(label_list)\n",
    "    \n",
    "    padding = \"max_length\"\n",
    "    pad_id = tokenizer.pad_token_id\n",
    "    batch['segments'] = []\n",
    "    tokenized = tokenizer(batch[\"text\"], padding=padding, truncation=True,\n",
    "                          max_length=4 * 512,\n",
    "                          add_special_tokens=False)  # prevent it from adding the cls and sep tokens twice\n",
    "    for ids in tokenized['input_ids']:\n",
    "        id_blocks = [ids[i:i + 512] for i in range(0, len(ids), 512) if\n",
    "                     ids[i] != pad_id]  # remove blocks containing only ids\n",
    "        id_blocks[-1] = [id for id in id_blocks[-1] if\n",
    "                         id != pad_id]  # remove remaining pad_tokens_ids from the last block\n",
    "        token_blocks = [tokenizer.convert_ids_to_tokens(ids) for ids in id_blocks]\n",
    "        string_blocks = [tokenizer.convert_tokens_to_string(tokens) for tokens in token_blocks]\n",
    "        batch['segments'].append(string_blocks)\n",
    "\n",
    "\n",
    "\n",
    "    tokenized = {'input_ids': [], 'attention_mask': [], 'token_type_ids': []}\n",
    "    for case in batch['segments']:\n",
    "        case_encodings = tokenizer(case[:4], padding=padding, truncation=True,\n",
    "                                   max_length=512, return_token_type_ids=True)\n",
    "        tokenized['input_ids'].append(append_zero_segments(case_encodings['input_ids'], pad_id))\n",
    "        tokenized['attention_mask'].append(append_zero_segments(case_encodings['attention_mask'], 0))\n",
    "        tokenized['token_type_ids'].append(append_zero_segments(case_encodings['token_type_ids'], 0))\n",
    "    del batch['segments']\n",
    "   \n",
    "    \n",
    "    if label_dict[\"label2id\"] is not None and \"label\" in batch:\n",
    "        tokenized[\"label\"] = [label_dict[\"label2id\"][l] for l in batch[\"label\"]]\n",
    "    return tokenized\n",
    "\n",
    "def preprocess_dataset(dataset):\n",
    "    return dataset.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        load_from_cache_file=False,\n",
    "        remove_columns=[col for col in dataset.column_names if not col == \"id\"],  # keep id for example-wise logging\n",
    "    )\n",
    "\n",
    "datasets = preprocess_dataset(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e536402",
   "metadata": {},
   "source": [
    "### Calculate Importance\n",
    "To reduce time, I only calculated the importance of the 500 words with the highest frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0adfac8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{72403: tensor(0.0014, device='cuda:1')}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "def get_logit(input_ids, attention_mask, token_type_ids):\n",
    "    input_ids = torch.tensor(input_ids).reshape(1, 4, 512).to('cuda:1')\n",
    "    attention_mask = torch.tensor(attention_mask).reshape(1, 4, 512).to('cuda:1')\n",
    "    token_type_ids = torch.tensor(token_type_ids).reshape(1, 4, 512).to('cuda:1')\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids, attention_mask, token_type_ids).logits\n",
    "    return logits\n",
    "\n",
    "\n",
    "def get_words(lang='fr', size=1):\n",
    "    ids2count = defaultdict(int)\n",
    "    for data in tqdm(datasets):\n",
    "        input_ids = []\n",
    "        for i in data['input_ids']:\n",
    "            input_ids += i\n",
    "        input_ids = input_ids[1:]\n",
    "        for ids in input_ids:\n",
    "            if ids == 1:\n",
    "                break\n",
    "            ids2count[ids] += 1\n",
    "    selected_ids = sorted(ids2count.items(), key=lambda x:x[1], reverse=True)[200:200+size]\n",
    "    selected_ids = [ids for ids, count in selected_ids]\n",
    "    words = tokenizer.convert_ids_to_tokens(selected_ids)\n",
    "    words = {token: ids for token, ids in zip(words, selected_ids)}\n",
    "    return words\n",
    "\n",
    "def dist(vec1, vec2):\n",
    "    return ((vec1 - vec2) ** 2).sum()\n",
    "\n",
    "def calc_importance():\n",
    "    negtive_pronouns = []\n",
    "    negtive_pronouns = tokenizer.convert_tokens_to_ids(negtive_pronouns)\n",
    "    words = get_words()\n",
    "    word2importance = defaultdict(list)\n",
    "    mask_id = tokenizer('<mask>')['input_ids'][1]\n",
    "    for token, ids in words.items():\n",
    "        for d in tqdm(datasets):\n",
    "            input_ids = []\n",
    "            for i in d['input_ids']:\n",
    "                input_ids += i\n",
    "            mask_input_ids = [i for i in input_ids]\n",
    "            attention_mask = d['attention_mask']\n",
    "            token_type_ids = d['token_type_ids']\n",
    "            try:\n",
    "                idx = input_ids.index(ids)\n",
    "            except Exception:\n",
    "                continue\n",
    "            mask_input_ids[idx] = mask_id\n",
    "            if idx > 0 and input_ids[idx-1] in negtive_pronouns:\n",
    "                mask_input_ids[idx-1] = mask_id\n",
    "            l1 = get_logit(input_ids, attention_mask, token_type_ids)\n",
    "            l2 = get_logit(mask_input_ids, attention_mask, token_type_ids)\n",
    "            word2importance[ids].append(dist(l1, l2))\n",
    "    word2importance = {word: sum(importance)/len(importance) for word, importance in word2importance.items()}\n",
    "    return word2importance\n",
    "\n",
    "\n",
    "word2importance = calc_importance()\n",
    "word2importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fca76ef",
   "metadata": {},
   "source": [
    "## Load Masked Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039b06dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForMaskedLM\n",
    "\n",
    "mlm = BertForMaskedLM.from_pretrained('xlm-roberta-base')\n",
    "words = sorted(word2importance.items(), key=lambda x:x[1], reverse=True)\n",
    "words = [word for word, imp in words]\n",
    "positive_words = []\n",
    "negtive_words = []\n",
    "for word in words:\n",
    "    for d in datasets:\n",
    "        input_ids = d['input_ids'][0]\n",
    "        try:\n",
    "            idx = input_ids.index(word)\n",
    "        except Exception:\n",
    "            continue\n",
    "        mask_ids = [ids for ids in input_ids]\n",
    "        mask_ids[idx] = 250001\n",
    "        logits = mlm(input_ids, ...).logits\n",
    "        possible_words = logits[0, idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de635ec9-ad76-4b2b-8f5a-0c23c54160b3",
   "metadata": {},
   "source": [
    "**This method is too difficult to implement, so I try to implement another one.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febf35fd-0679-433d-a479-59cacad11779",
   "metadata": {},
   "source": [
    "# Visualize Attention Layers Of Fited Bert.\n",
    "1. load model\n",
    "2. load text data\n",
    "3. explain model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "403e200c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all pacakages used in teacher's scripts\n",
    "import dataclasses\n",
    "import shutil\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from enum import Enum\n",
    "import json\n",
    "\n",
    "import wandb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    precision_recall_fscore_support,\n",
    "    multilabel_confusion_matrix,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    balanced_accuracy_score,\n",
    "    roc_auc_score,\n",
    "    average_precision_score, matthews_corrcoef\n",
    ")\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.utils import compute_class_weight\n",
    "\n",
    "import torch\n",
    "from sklearn.utils.extmath import softmax\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "import transformers\n",
    "import transformers.adapters.composition as ac\n",
    "from transformers import (\n",
    "    AdapterConfig,\n",
    "    AdapterTrainer,\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    EvalPrediction,\n",
    "    EarlyStoppingCallback,\n",
    "    HfArgumentParser,\n",
    "    MultiLingAdapterArguments,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    "    set_seed, TrainerCallback, XLMRobertaTokenizer,\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint, is_main_process\n",
    "from transformers.utils import check_min_version\n",
    "\n",
    "from root import DATA_DIR, AUGMENTED_DIR\n",
    "from utils.custom_callbacks import CustomWandbCallback\n",
    "from long import LongBert\n",
    "from arguments.data_arguments import DataArguments, ProblemType, SegmentationType, DataAugmentationType, LegalArea, \\\n",
    "    OriginCanton, SubDataset, OriginRegion, Jurisdiction\n",
    "from hierarchical.hier_bert.configuration_hier_bert import HierBertConfig\n",
    "from hierarchical.hier_bert.modeling_hier_bert import HierBertForSequenceClassification\n",
    "from hierarchical.hier_camembert.configuration_hier_camembert import HierCamembertConfig\n",
    "from hierarchical.hier_camembert.modeling_hier_camembert import HierCamembertForSequenceClassification\n",
    "from hierarchical.hier_roberta.configuration_hier_roberta import HierRobertaConfig\n",
    "from hierarchical.hier_roberta.modeling_hier_roberta import HierRobertaForSequenceClassification\n",
    "from hierarchical.hier_xlm_roberta.configuration_hier_xlm_roberta import HierXLMRobertaConfig\n",
    "from hierarchical.hier_xlm_roberta.modeling_hier_xlm_roberta import HierXLMRobertaForSequenceClassification\n",
    "from arguments.model_arguments import ModelArguments, LabelImbalanceMethod, LongInputBertType, TrainType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0c14cf-5e2d-4454-83ef-996b794b1406",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f61356c-d6ec-4842-9b87-1061308ebf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_DIR / 'de' / 'labels.json', 'r') as f:\n",
    "    label_dict = json.load(f)\n",
    "    label_dict['id2label'] = {int(k): v for k, v in label_dict['id2label'].items()}\n",
    "    label_dict['label2id'] = {k: int(v) for k, v in label_dict['label2id'].items()}\n",
    "    label_list = list(label_dict[\"label2id\"].keys())\n",
    "num_labels = len(label_list)\n",
    "    \n",
    "    \n",
    "def baseline():\n",
    "\n",
    "\n",
    "    model_class = AutoModelForSequenceClassification\n",
    "\n",
    "    config = AutoConfig.from_pretrained(\n",
    "                    'xlm-roberta-base',\n",
    "                    num_labels=num_labels,\n",
    "                    id2label=label_dict[\"id2label\"],\n",
    "                    label2id=label_dict[\"label2id\"],\n",
    "                    finetuning_task=\"text-classification\",\n",
    "                    problem_type='single_label_classification',\n",
    "                    cache_dir=None,\n",
    "                    revision='main',\n",
    "                    use_auth_token=None,\n",
    "                    max_segments=4,\n",
    "                    max_segment_length=512,\n",
    "                    segment_encoder_type=\"transformer\",\n",
    "                    output_attentions=True\n",
    "                )\n",
    "\n",
    "\n",
    "    if config.model_type == 'bert':\n",
    "        config_class = HierBertConfig\n",
    "        model_class = HierBertForSequenceClassification\n",
    "    if config.model_type == 'roberta':\n",
    "        config_class = HierRobertaConfig\n",
    "        model_class = HierRobertaForSequenceClassification\n",
    "    if config.model_type == 'xlm-roberta':\n",
    "        config_class = HierXLMRobertaConfig\n",
    "        model_class = HierXLMRobertaForSequenceClassification\n",
    "    if config.model_type == 'camembert':\n",
    "        config_class = HierCamembertConfig\n",
    "        model_class = HierCamembertForSequenceClassification\n",
    "    config = config_class(**config.to_dict())\n",
    "\n",
    "\n",
    "    model = model_class.from_pretrained(\n",
    "        'xlm-roberta-base',\n",
    "        from_tf=bool(\".ckpt\" in 'xlm-roberta-base'),\n",
    "        config=config,\n",
    "        cache_dir=None,\n",
    "        revision='main',\n",
    "        use_auth_token=None,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def load_model(model, folder):\n",
    "    model_path = Path(f'{folder}/model.bin')\n",
    "    if model_path.exists():\n",
    "        model.load_state_dict(torch.load(model_path, map_location='cuda:1'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf02976f-4ac4-437b-bda2-a20eca3035f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing HierXLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing HierXLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing HierXLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of HierXLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['segment_encoder.layers.1.self_attn.out_proj.weight', 'segment_encoder.layers.1.norm2.bias', 'segment_encoder.layers.0.linear1.bias', 'segment_encoder.norm.bias', 'segment_encoder.layers.1.linear2.weight', 'segment_encoder.layers.1.self_attn.out_proj.bias', 'segment_encoder.layers.0.norm2.weight', 'classifier.weight', 'seg_pos_embeddings.weight', 'segment_encoder.layers.1.linear1.bias', 'classifier.bias', 'segment_encoder.layers.1.norm1.bias', 'segment_encoder.layers.0.self_attn.out_proj.weight', 'segment_encoder.layers.0.linear2.weight', 'segment_encoder.layers.0.self_attn.out_proj.bias', 'segment_encoder.layers.0.self_attn.in_proj_weight', 'segment_encoder.layers.0.linear2.bias', 'segment_encoder.layers.1.norm2.weight', 'segment_encoder.layers.0.norm1.weight', 'segment_encoder.layers.0.linear1.weight', 'segment_encoder.layers.1.self_attn.in_proj_bias', 'segment_encoder.layers.1.linear1.weight', 'segment_encoder.layers.0.norm2.bias', 'segment_encoder.norm.weight', 'segment_encoder.layers.0.self_attn.in_proj_bias', 'segment_encoder.layers.1.linear2.bias', 'segment_encoder.layers.0.norm1.bias', 'segment_encoder.layers.1.self_attn.in_proj_weight', 'segment_encoder.layers.1.norm1.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = baseline()\n",
    "load_model(model, 'sjp/teacher_model')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "        'xlm-roberta-base',\n",
    "        do_lower_case=False,\n",
    "        cache_dir=None,\n",
    "        use_fast=True,\n",
    "        revision='main',\n",
    "        use_auth_token=None,\n",
    "        return_token_type_ids=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fae2fa22-a2d7-43b0-879c-d78d3fd80e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = model.roberta\n",
    "tokenizer.model_max_length = 2048"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73aa981-6e28-4319-98b5-9773aa4fd994",
   "metadata": {},
   "source": [
    "# Visualize Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb0ed9e-2ad7-4056-a2b2-05b4e05a7a4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\"></script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "      \n",
       "        <div id=\"bertviz-7f5962ffa47b4ecc98aabfba2087b888\" style=\"font-family:'Helvetica Neue', Helvetica, Arial, sans-serif;\">\n",
       "            <span style=\"user-select:none\">\n",
       "                \n",
       "            </span>\n",
       "            <div id='vis'></div>\n",
       "        </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from bertviz import model_view\n",
    "from transformers import utils\n",
    "\n",
    "\n",
    "utils.logging.set_verbosity_error()  \n",
    "\n",
    "\n",
    "input_text = \"A.- Der 1945 geborene S._ meldete sich am 20. Januar 1997 bei der Invalidenversicherung wegen Schmerzen im Bereich des Rückens, der Hüfte und des rechten Beins zum Leistungsbezug an.\"\n",
    "inputs = tokenizer.encode(input_text, return_tensors='pt')  # Tokenize input text\n",
    "outputs = bert(inputs)  # Run model\n",
    "attention = outputs[-1]  # Retrieve attention from model outputs\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs[0])  # Convert input ids to token strings\n",
    "model_view(attention, tokens)  # Display model view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e84a5ce-a82f-4efb-a667-5668d42e8ae3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
